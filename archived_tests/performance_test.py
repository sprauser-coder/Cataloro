#!/usr/bin/env python3
"""
COMPREHENSIVE PERFORMANCE VERIFICATION - Post-Optimization Testing
Testing system-wide performance optimizations and database improvements
"""

import asyncio
import aiohttp
import time
import json
import statistics
from datetime import datetime
from typing import Dict, List, Any

# Test Configuration
BACKEND_URL = "https://marketplace-perf-1.preview.emergentagent.com/api"
PERFORMANCE_TARGET_MS = 1000  # All endpoints should respond in <1 second
CRITICAL_PERFORMANCE_TARGET_MS = 500  # Critical endpoints should respond in <500ms
ADMIN_CALCULATIONS_TARGET_MS = 1000  # Admin catalyst calculations should be <1s

# Test User IDs (from test_result.md)
DEMO_USER_ID = "68bfff790e4e46bc28d43631"
ADMIN_USER_ID = "admin_user_1"
TARGET_LISTING = "walker351631A"

class PerformanceVerificationTester:
    def __init__(self):
        self.session = None
        self.test_results = []
        self.admin_token = None
        self.demo_token = None
        
    async def setup(self):
        """Initialize HTTP session"""
        self.session = aiohttp.ClientSession()
        
    async def cleanup(self):
        """Cleanup HTTP session"""
        if self.session:
            await self.session.close()
    
    async def make_request(self, endpoint: str, method: str = "GET", params: Dict = None, data: Dict = None, headers: Dict = None) -> Dict:
        """Make HTTP request and measure response time"""
        start_time = time.time()
        
        try:
            request_kwargs = {}
            if params:
                request_kwargs['params'] = params
            if data:
                request_kwargs['json'] = data
            if headers:
                request_kwargs['headers'] = headers
            
            async with self.session.request(method, f"{BACKEND_URL}{endpoint}", **request_kwargs) as response:
                end_time = time.time()
                response_time_ms = (end_time - start_time) * 1000
                
                try:
                    response_data = await response.json()
                except:
                    response_data = await response.text()
                
                return {
                    "success": response.status in [200, 201],
                    "response_time_ms": response_time_ms,
                    "data": response_data,
                    "status": response.status
                }
        except Exception as e:
            end_time = time.time()
            response_time_ms = (end_time - start_time) * 1000
            return {
                "success": False,
                "response_time_ms": response_time_ms,
                "error": str(e),
                "status": 0
            }
    
    async def authenticate_users(self) -> Dict:
        """Authenticate demo and admin users"""
        print("üîê Authenticating test users...")
        
        # Authenticate demo user
        demo_login = {
            "email": "demo@cataloro.com",
            "password": "demo_password"
        }
        
        demo_result = await self.make_request("/auth/login", "POST", data=demo_login)
        if demo_result["success"]:
            self.demo_token = demo_result["data"].get("token", "")
            print(f"  ‚úÖ Demo user authenticated: {demo_result['response_time_ms']:.0f}ms")
        else:
            print(f"  ‚ùå Demo user authentication failed: {demo_result.get('error')}")
        
        # Authenticate admin user
        admin_login = {
            "email": "admin@cataloro.com",
            "password": "admin_password"
        }
        
        admin_result = await self.make_request("/auth/login", "POST", data=admin_login)
        if admin_result["success"]:
            self.admin_token = admin_result["data"].get("token", "")
            print(f"  ‚úÖ Admin user authenticated: {admin_result['response_time_ms']:.0f}ms")
        else:
            print(f"  ‚ùå Admin user authentication failed: {admin_result.get('error')}")
        
        return {
            "demo_authenticated": demo_result["success"],
            "admin_authenticated": admin_result["success"],
            "demo_response_time_ms": demo_result["response_time_ms"],
            "admin_response_time_ms": admin_result["response_time_ms"]
        }
    
    async def test_database_performance(self) -> Dict:
        """Test all optimized API endpoints with response time measurements"""
        print("üöÄ Testing database performance with optimized endpoints...")
        
        endpoint_tests = []
        
        # Test critical endpoints that were previously slow (5-6 seconds)
        critical_endpoints = [
            {
                "name": "Demo User My-Listings",
                "endpoint": f"/user/my-listings/{DEMO_USER_ID}",
                "target_ms": CRITICAL_PERFORMANCE_TARGET_MS,
                "description": "Should be <500ms (was 5+ seconds)"
            },
            {
                "name": "Admin User My-Listings", 
                "endpoint": f"/user/my-listings/{ADMIN_USER_ID}",
                "target_ms": CRITICAL_PERFORMANCE_TARGET_MS,
                "description": "Should be <500ms (was 5+ seconds)"
            },
            {
                "name": "Admin Catalyst Calculations",
                "endpoint": "/admin/catalyst/calculations",
                "target_ms": ADMIN_CALCULATIONS_TARGET_MS,
                "description": "Should be <1s (was 6+ seconds)"
            }
        ]
        
        # Test standard endpoints
        standard_endpoints = [
            {
                "name": "Browse Listings",
                "endpoint": "/marketplace/browse",
                "target_ms": PERFORMANCE_TARGET_MS,
                "description": "Main browse endpoint"
            },
            {
                "name": "Health Check",
                "endpoint": "/health",
                "target_ms": 200,
                "description": "System health"
            },
            {
                "name": "Performance Metrics",
                "endpoint": "/admin/performance",
                "target_ms": PERFORMANCE_TARGET_MS,
                "description": "Performance monitoring"
            }
        ]
        
        all_endpoints = critical_endpoints + standard_endpoints
        
        for endpoint_test in all_endpoints:
            print(f"  Testing {endpoint_test['name']}...")
            
            # Run multiple calls to get average performance
            response_times = []
            success_count = 0
            
            for i in range(3):  # 3 calls per endpoint
                result = await self.make_request(endpoint_test["endpoint"])
                response_times.append(result["response_time_ms"])
                if result["success"]:
                    success_count += 1
            
            avg_response_time = statistics.mean(response_times)
            meets_target = avg_response_time < endpoint_test["target_ms"]
            
            endpoint_result = {
                "endpoint_name": endpoint_test["name"],
                "endpoint_path": endpoint_test["endpoint"],
                "avg_response_time_ms": avg_response_time,
                "min_response_time_ms": min(response_times),
                "max_response_time_ms": max(response_times),
                "target_ms": endpoint_test["target_ms"],
                "meets_target": meets_target,
                "success_rate": (success_count / 3) * 100,
                "description": endpoint_test["description"],
                "is_critical": endpoint_test in critical_endpoints
            }
            
            endpoint_tests.append(endpoint_result)
            
            status = "‚úÖ" if meets_target and success_count == 3 else "‚ùå"
            print(f"    {status} {avg_response_time:.0f}ms (target: {endpoint_test['target_ms']}ms)")
        
        # Calculate overall performance metrics
        critical_tests = [t for t in endpoint_tests if t["is_critical"]]
        critical_success = all(t["meets_target"] and t["success_rate"] == 100 for t in critical_tests)
        
        overall_success = all(t["meets_target"] and t["success_rate"] == 100 for t in endpoint_tests)
        avg_improvement = self.calculate_performance_improvement(endpoint_tests)
        
        return {
            "test_name": "Database Performance Testing",
            "total_endpoints_tested": len(endpoint_tests),
            "critical_endpoints_optimized": critical_success,
            "all_endpoints_meet_targets": overall_success,
            "avg_performance_improvement_percent": avg_improvement,
            "detailed_results": endpoint_tests
        }
    
    async def test_database_connection_health(self) -> Dict:
        """Verify MongoDB connection is stable and using correct database"""
        print("üóÑÔ∏è Testing database connection health...")
        
        health_tests = []
        
        # Test health endpoint
        health_result = await self.make_request("/health")
        health_tests.append({
            "test": "Health Check",
            "success": health_result["success"],
            "response_time_ms": health_result["response_time_ms"],
            "data": health_result.get("data", {})
        })
        
        # Test performance metrics (includes database stats)
        perf_result = await self.make_request("/admin/performance")
        db_stats = {}
        if perf_result["success"]:
            perf_data = perf_result.get("data", {})
            db_stats = perf_data.get("database", {})
        
        health_tests.append({
            "test": "Database Statistics",
            "success": perf_result["success"],
            "response_time_ms": perf_result["response_time_ms"],
            "database_name": db_stats.get("name", "unknown"),
            "collections_count": db_stats.get("collections", 0),
            "has_indexes": "index_size" in db_stats
        })
        
        # Test data queries return expected data
        browse_result = await self.make_request("/marketplace/browse")
        has_data = browse_result["success"] and len(browse_result.get("data", [])) > 0
        
        health_tests.append({
            "test": "Data Query Validation",
            "success": browse_result["success"],
            "response_time_ms": browse_result["response_time_ms"],
            "has_listings_data": has_data,
            "listings_count": len(browse_result.get("data", []))
        })
        
        # Check for "ns does not exist" errors by testing user endpoints
        demo_listings_result = await self.make_request(f"/user/my-listings/{DEMO_USER_ID}")
        no_ns_errors = demo_listings_result["success"] or "ns does not exist" not in str(demo_listings_result.get("error", ""))
        
        health_tests.append({
            "test": "Namespace Error Check",
            "success": demo_listings_result["success"],
            "response_time_ms": demo_listings_result["response_time_ms"],
            "no_ns_errors": no_ns_errors,
            "error": demo_listings_result.get("error", "")
        })
        
        # Calculate overall health
        successful_tests = [t for t in health_tests if t["success"]]
        connection_stable = len(successful_tests) >= 3  # At least 3/4 tests should pass
        
        return {
            "test_name": "Database Connection Health",
            "connection_stable": connection_stable,
            "successful_health_checks": len(successful_tests),
            "total_health_checks": len(health_tests),
            "no_namespace_errors": no_ns_errors,
            "database_responding": db_stats.get("name") == "cataloro_marketplace",
            "detailed_health_tests": health_tests
        }
    
    async def test_memory_usage_verification(self) -> Dict:
        """Check that service restarts cleared memory leaks"""
        print("üíæ Testing memory usage verification...")
        
        # Test performance metrics endpoint for memory information
        perf_result = await self.make_request("/admin/performance")
        
        memory_tests = []
        
        if perf_result["success"]:
            perf_data = perf_result.get("data", {})
            
            # Check optimizations status
            optimizations = perf_data.get("optimizations", {})
            memory_optimized = "memory" in str(optimizations).lower() or "Low-memory operations" in str(optimizations)
            
            memory_tests.append({
                "test": "Memory Optimization Status",
                "success": True,
                "memory_optimizations_active": memory_optimized,
                "optimization_details": optimizations
            })
            
            # Check scalability metrics
            scalability = perf_data.get("scalability", {})
            memory_usage_info = scalability.get("memory_usage", "")
            
            memory_tests.append({
                "test": "Memory Usage Information",
                "success": True,
                "memory_usage_status": memory_usage_info,
                "scalability_info": scalability
            })
        
        # Test system responsiveness (indicator of memory health)
        response_times = []
        for i in range(5):
            health_result = await self.make_request("/health")
            response_times.append(health_result["response_time_ms"])
        
        avg_response_time = statistics.mean(response_times)
        consistent_performance = max(response_times) - min(response_times) < 100  # Less than 100ms variance
        
        memory_tests.append({
            "test": "System Responsiveness",
            "success": True,
            "avg_response_time_ms": avg_response_time,
            "response_time_variance": max(response_times) - min(response_times),
            "consistent_performance": consistent_performance,
            "indicates_healthy_memory": avg_response_time < 200 and consistent_performance
        })
        
        # Overall memory health assessment
        memory_healthy = all(t.get("success", False) for t in memory_tests)
        no_memory_leaks = consistent_performance and avg_response_time < 500
        
        return {
            "test_name": "Memory Usage Verification",
            "memory_optimizations_detected": any(t.get("memory_optimizations_active") for t in memory_tests),
            "system_responsive": avg_response_time < 200,
            "performance_consistent": consistent_performance,
            "no_memory_leak_indicators": no_memory_leaks,
            "service_restart_effective": memory_healthy and no_memory_leaks,
            "detailed_memory_tests": memory_tests
        }
    
    async def test_api_response_consistency(self) -> Dict:
        """Test demo user and admin user listings consistency"""
        print("üîÑ Testing API response consistency...")
        
        consistency_tests = []
        
        # Test demo user listings (68bfff790e4e46bc28d43631)
        print(f"  Testing demo user listings ({DEMO_USER_ID})...")
        demo_results = []
        for i in range(3):
            result = await self.make_request(f"/user/my-listings/{DEMO_USER_ID}")
            demo_results.append(result)
        
        demo_consistent = all(r["success"] for r in demo_results)
        demo_data_consistent = True
        if demo_consistent:
            # Check if all calls return same number of listings
            listing_counts = [len(r.get("data", [])) for r in demo_results]
            demo_data_consistent = len(set(listing_counts)) == 1  # All counts should be the same
        
        demo_avg_time = statistics.mean([r["response_time_ms"] for r in demo_results])
        demo_fast = demo_avg_time < CRITICAL_PERFORMANCE_TARGET_MS
        
        consistency_tests.append({
            "user": "Demo User",
            "user_id": DEMO_USER_ID,
            "all_calls_successful": demo_consistent,
            "data_consistent": demo_data_consistent,
            "avg_response_time_ms": demo_avg_time,
            "meets_performance_target": demo_fast,
            "listing_counts": [len(r.get("data", [])) for r in demo_results] if demo_consistent else []
        })
        
        print(f"    Demo user: {demo_avg_time:.0f}ms, consistent: {demo_data_consistent}")
        
        # Test admin user listings (admin_user_1) - should include walker351631A
        print(f"  Testing admin user listings ({ADMIN_USER_ID})...")
        admin_results = []
        for i in range(3):
            result = await self.make_request(f"/user/my-listings/{ADMIN_USER_ID}")
            admin_results.append(result)
        
        admin_consistent = all(r["success"] for r in admin_results)
        admin_data_consistent = True
        walker_found = False
        
        if admin_consistent:
            # Check if all calls return same number of listings
            listing_counts = [len(r.get("data", [])) for r in admin_results]
            admin_data_consistent = len(set(listing_counts)) == 1
            
            # Check if walker351631A is found in admin listings
            for result in admin_results:
                listings = result.get("data", [])
                for listing in listings:
                    if TARGET_LISTING.lower() in listing.get("title", "").lower():
                        walker_found = True
                        break
                if walker_found:
                    break
        
        admin_avg_time = statistics.mean([r["response_time_ms"] for r in admin_results])
        admin_fast = admin_avg_time < CRITICAL_PERFORMANCE_TARGET_MS
        
        consistency_tests.append({
            "user": "Admin User",
            "user_id": ADMIN_USER_ID,
            "all_calls_successful": admin_consistent,
            "data_consistent": admin_data_consistent,
            "avg_response_time_ms": admin_avg_time,
            "meets_performance_target": admin_fast,
            "walker351631A_found": walker_found,
            "listing_counts": [len(r.get("data", [])) for r in admin_results] if admin_consistent else []
        })
        
        print(f"    Admin user: {admin_avg_time:.0f}ms, walker351631A found: {walker_found}")
        
        # Overall consistency assessment
        all_consistent = all(t["all_calls_successful"] and t["data_consistent"] for t in consistency_tests)
        all_fast = all(t["meets_performance_target"] for t in consistency_tests)
        
        return {
            "test_name": "API Response Consistency",
            "all_users_consistent": all_consistent,
            "all_responses_fast": all_fast,
            "demo_user_working": demo_consistent and demo_fast,
            "admin_user_working": admin_consistent and admin_fast,
            "walker351631A_visible": walker_found,
            "detailed_consistency_tests": consistency_tests
        }
    
    async def test_cross_platform_data_loading(self) -> Dict:
        """Verify both mobile and desktop can access listings consistently"""
        print("üì±üíª Testing cross-platform data loading...")
        
        platform_tests = []
        
        # Simulate mobile request (with mobile user agent)
        mobile_headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15"
        }
        
        # Simulate desktop request (with desktop user agent)
        desktop_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        # Test browse endpoint from both platforms
        mobile_browse = await self.make_request("/marketplace/browse", headers=mobile_headers)
        desktop_browse = await self.make_request("/marketplace/browse", headers=desktop_headers)
        
        mobile_success = mobile_browse["success"]
        desktop_success = desktop_browse["success"]
        
        # Compare data consistency
        data_consistent = False
        if mobile_success and desktop_success:
            mobile_listings = mobile_browse.get("data", [])
            desktop_listings = desktop_browse.get("data", [])
            data_consistent = len(mobile_listings) == len(desktop_listings)
            
            # Check first few listings for consistency
            if data_consistent and len(mobile_listings) > 0:
                for i in range(min(3, len(mobile_listings))):
                    mobile_listing = mobile_listings[i]
                    desktop_listing = desktop_listings[i]
                    if mobile_listing.get("id") != desktop_listing.get("id"):
                        data_consistent = False
                        break
        
        platform_tests.append({
            "test": "Browse Endpoint Cross-Platform",
            "mobile_success": mobile_success,
            "desktop_success": desktop_success,
            "mobile_response_time_ms": mobile_browse["response_time_ms"],
            "desktop_response_time_ms": desktop_browse["response_time_ms"],
            "data_consistent": data_consistent,
            "mobile_listings_count": len(mobile_browse.get("data", [])),
            "desktop_listings_count": len(desktop_browse.get("data", []))
        })
        
        # Test messaging endpoints performance
        mobile_messages = await self.make_request(f"/user/{DEMO_USER_ID}/messages", headers=mobile_headers)
        desktop_messages = await self.make_request(f"/user/{DEMO_USER_ID}/messages", headers=desktop_headers)
        
        messages_consistent = False
        if mobile_messages["success"] and desktop_messages["success"]:
            mobile_msg_count = len(mobile_messages.get("data", []))
            desktop_msg_count = len(desktop_messages.get("data", []))
            messages_consistent = mobile_msg_count == desktop_msg_count
        
        platform_tests.append({
            "test": "Messages Endpoint Cross-Platform",
            "mobile_success": mobile_messages["success"],
            "desktop_success": desktop_messages["success"],
            "mobile_response_time_ms": mobile_messages["response_time_ms"],
            "desktop_response_time_ms": desktop_messages["response_time_ms"],
            "data_consistent": messages_consistent,
            "mobile_messages_count": len(mobile_messages.get("data", [])),
            "desktop_messages_count": len(desktop_messages.get("data", []))
        })
        
        # Overall cross-platform assessment
        all_platforms_working = all(t["mobile_success"] and t["desktop_success"] for t in platform_tests)
        all_data_consistent = all(t["data_consistent"] for t in platform_tests)
        
        return {
            "test_name": "Cross-Platform Data Loading",
            "mobile_desktop_consistency": all_data_consistent,
            "all_platforms_accessible": all_platforms_working,
            "browse_cross_platform_working": platform_tests[0]["data_consistent"],
            "messaging_cross_platform_working": platform_tests[1]["data_consistent"],
            "detailed_platform_tests": platform_tests
        }
    
    async def test_index_effectiveness(self) -> Dict:
        """Test queries with seller_id filtering and other indexed fields"""
        print("üìä Testing database index effectiveness...")
        
        index_tests = []
        
        # Test seller_id filtering (should use indexes)
        seller_filter_result = await self.make_request("/marketplace/browse", params={"seller_id": DEMO_USER_ID})
        seller_fast = seller_filter_result["response_time_ms"] < PERFORMANCE_TARGET_MS
        
        index_tests.append({
            "test": "Seller ID Filtering",
            "success": seller_filter_result["success"],
            "response_time_ms": seller_filter_result["response_time_ms"],
            "uses_index_effectively": seller_fast,
            "results_count": len(seller_filter_result.get("data", []))
        })
        
        # Test status-based queries (should use indexes)
        status_filter_result = await self.make_request("/marketplace/browse", params={"status": "active"})
        status_fast = status_filter_result["response_time_ms"] < PERFORMANCE_TARGET_MS
        
        index_tests.append({
            "test": "Status-Based Filtering",
            "success": status_filter_result["success"],
            "response_time_ms": status_filter_result["response_time_ms"],
            "uses_index_effectively": status_fast,
            "results_count": len(status_filter_result.get("data", []))
        })
        
        # Test created_at sorting (should use indexes)
        sort_result = await self.make_request("/marketplace/browse", params={"sort": "newest"})
        sort_fast = sort_result["response_time_ms"] < PERFORMANCE_TARGET_MS
        
        index_tests.append({
            "test": "Created At Sorting",
            "success": sort_result["success"],
            "response_time_ms": sort_result["response_time_ms"],
            "uses_index_effectively": sort_fast,
            "results_count": len(sort_result.get("data", []))
        })
        
        # Test complex query (multiple filters)
        complex_result = await self.make_request("/marketplace/browse", params={
            "status": "active",
            "price_min": 50,
            "price_max": 500,
            "sort": "newest"
        })
        complex_fast = complex_result["response_time_ms"] < PERFORMANCE_TARGET_MS
        
        index_tests.append({
            "test": "Complex Multi-Filter Query",
            "success": complex_result["success"],
            "response_time_ms": complex_result["response_time_ms"],
            "uses_index_effectively": complex_fast,
            "results_count": len(complex_result.get("data", []))
        })
        
        # Check performance metrics for index information
        perf_result = await self.make_request("/admin/performance")
        index_info = {}
        if perf_result["success"]:
            collections = perf_result.get("data", {}).get("collections", {})
            index_info = {
                "listings_indexes": collections.get("listings", {}).get("index_count", 0),
                "users_indexes": collections.get("users", {}).get("index_count", 0),
                "tenders_indexes": collections.get("tenders", {}).get("index_count", 0)
            }
        
        # Overall index effectiveness
        all_queries_fast = all(t["uses_index_effectively"] for t in index_tests)
        sufficient_indexes = sum(index_info.values()) > 10  # Should have many indexes
        
        return {
            "test_name": "Index Effectiveness Testing",
            "all_queries_use_indexes_effectively": all_queries_fast,
            "sufficient_indexes_created": sufficient_indexes,
            "total_indexes_detected": sum(index_info.values()),
            "index_distribution": index_info,
            "detailed_index_tests": index_tests
        }
    
    def calculate_performance_improvement(self, endpoint_tests: List[Dict]) -> float:
        """Calculate average performance improvement percentage"""
        # Assume previous performance was 5-6 seconds for critical endpoints
        improvements = []
        
        for test in endpoint_tests:
            if test["is_critical"]:
                # Critical endpoints were 5000-6000ms, now should be <500ms
                old_time = 5500  # Average of 5-6 seconds
                new_time = test["avg_response_time_ms"]
                improvement = ((old_time - new_time) / old_time) * 100
                improvements.append(improvement)
            else:
                # Standard endpoints assume 50% improvement
                improvements.append(50.0)
        
        return statistics.mean(improvements) if improvements else 0
    
    async def run_comprehensive_performance_test(self) -> Dict:
        """Run all performance verification tests"""
        print("üöÄ Starting Comprehensive Performance Verification Testing")
        print("=" * 70)
        
        await self.setup()
        
        try:
            # Authenticate users first
            auth_results = await self.authenticate_users()
            
            # Run all test suites
            db_performance = await self.test_database_performance()
            db_health = await self.test_database_connection_health()
            memory_verification = await self.test_memory_usage_verification()
            api_consistency = await self.test_api_response_consistency()
            cross_platform = await self.test_cross_platform_data_loading()
            index_effectiveness = await self.test_index_effectiveness()
            
            # Compile overall results
            all_results = {
                "test_timestamp": datetime.now().isoformat(),
                "test_type": "Comprehensive Performance Verification",
                "authentication": auth_results,
                "database_performance": db_performance,
                "database_connection_health": db_health,
                "memory_usage_verification": memory_verification,
                "api_response_consistency": api_consistency,
                "cross_platform_data_loading": cross_platform,
                "index_effectiveness": index_effectiveness
            }
            
            # Calculate overall success metrics based on success criteria
            success_criteria = [
                db_performance.get("critical_endpoints_optimized", False),  # <500ms for critical endpoints
                db_performance.get("all_endpoints_meet_targets", False),    # <1s for all endpoints
                db_health.get("connection_stable", False),                  # Stable MongoDB connection
                memory_verification.get("service_restart_effective", False), # Memory leaks cleared
                api_consistency.get("walker351631A_visible", False),       # Walker351631A visible
                api_consistency.get("demo_user_working", False),            # Demo user listings work
                cross_platform.get("mobile_desktop_consistency", False),   # Cross-platform consistency
                index_effectiveness.get("all_queries_use_indexes_effectively", False) # Indexes working
            ]
            
            overall_success_rate = sum(success_criteria) / len(success_criteria) * 100
            
            all_results["summary"] = {
                "overall_success_rate": overall_success_rate,
                "performance_optimization_successful": overall_success_rate >= 87.5,  # 7/8 criteria
                "critical_endpoints_under_500ms": db_performance.get("critical_endpoints_optimized", False),
                "all_endpoints_under_1s": db_performance.get("all_endpoints_meet_targets", False),
                "database_connection_healthy": db_health.get("connection_stable", False),
                "memory_leaks_cleared": memory_verification.get("service_restart_effective", False),
                "walker351631A_now_visible": api_consistency.get("walker351631A_visible", False),
                "demo_user_listings_fast": api_consistency.get("demo_user_working", False),
                "cross_platform_consistent": cross_platform.get("mobile_desktop_consistency", False),
                "database_indexes_effective": index_effectiveness.get("all_queries_use_indexes_effectively", False),
                "system_ready_for_production": overall_success_rate >= 87.5
            }
            
            return all_results
            
        finally:
            await self.cleanup()

async def main():
    """Run the comprehensive performance verification test"""
    tester = PerformanceVerificationTester()
    results = await tester.run_comprehensive_performance_test()
    
    # Print summary
    print("\n" + "=" * 70)
    print("üìä PERFORMANCE VERIFICATION SUMMARY")
    print("=" * 70)
    
    summary = results["summary"]
    print(f"Overall Success Rate: {summary['overall_success_rate']:.1f}%")
    print(f"System Ready for Production: {'‚úÖ YES' if summary['system_ready_for_production'] else '‚ùå NO'}")
    print()
    
    # Print key metrics
    print("üéØ SUCCESS CRITERIA:")
    criteria = [
        ("Critical endpoints <500ms", summary['critical_endpoints_under_500ms']),
        ("All endpoints <1s", summary['all_endpoints_under_1s']),
        ("Database connection healthy", summary['database_connection_healthy']),
        ("Memory leaks cleared", summary['memory_leaks_cleared']),
        ("Walker351631A visible", summary['walker351631A_now_visible']),
        ("Demo user listings fast", summary['demo_user_listings_fast']),
        ("Cross-platform consistent", summary['cross_platform_consistent']),
        ("Database indexes effective", summary['database_indexes_effective'])
    ]
    
    for criterion, passed in criteria:
        status = "‚úÖ" if passed else "‚ùå"
        print(f"  {status} {criterion}")
    
    print("\n" + "=" * 70)
    
    # Save detailed results
    with open("/app/performance_test_results.json", "w") as f:
        json.dump(results, f, indent=2, default=str)
    
    print("üìÑ Detailed results saved to: /app/performance_test_results.json")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())